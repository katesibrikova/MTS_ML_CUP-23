{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d813e4fa",
   "metadata": {},
   "source": [
    "# Raw обработка собраных titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4b888281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b404e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#собираю названия всех папок по которым разложены данные\n",
    "path = r'./urls/output' \n",
    "all_holders = glob.glob(os.path.join(path,'*'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ecc2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#собираю названия всех путей к файлам по которым разложены данные (с названиями файлов)\n",
    "all_files=[]\n",
    "for i in all_holders:\n",
    "    all_files.append(glob.glob(os.path.join(i,'*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f443f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#собираю все ulr и пути\n",
    "urls=[]\n",
    "roots_unfolded=[]\n",
    "for i in range(len(all_holders)):\n",
    "    names=[]\n",
    "    for j in all_files[i]:\n",
    "        names.append(j.replace((all_holders[i]+\"/\"),\"\"))\n",
    "        roots_unfolded.append(j)\n",
    "    urls.extend(names)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adfbd391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 27min 23s, sys: 6min 55s, total: 1h 34min 19s\n",
      "Wall time: 1h 58min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "titles=[]\n",
    "err=[]\n",
    "for i in range(len(roots_unfolded)):\n",
    "    try:\n",
    "        titles.append(BeautifulSoup(open(roots_unfolded[i])).title.string)\n",
    "        #print(BeautifulSoup(open(roots_unfolded[i])).title.string)\n",
    "    except:\n",
    "        err.append(i)\n",
    "        titles.append(urls[i])\n",
    "       # print(urls[i])\n",
    "    \n",
    "\n",
    "#cобрали все тайтлы, а где была ошибка - прометили error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "161ff30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Собираем url и тайтлы в один датасет\n",
    "df = pd.DataFrame(zip(urls,titles))\n",
    "df=df.reset_index()\n",
    "df['is_parced'] = df['index'].apply(lambda x: ~(x in err))\n",
    "df.columns = ['index','url','title','is_parced']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e178e2c",
   "metadata": {},
   "source": [
    "Там, где отсутсвует тайтл - вместо него указываем адрес + делаем в столбце is_parced соотвествующую отметку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f3af382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['title'].isna(),'is_parced'] = -2\n",
    "df.loc[df['title'].isna(),'title'] = df.loc[df['title'].isna(),'url']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec171fa3",
   "metadata": {},
   "source": [
    "Создаю вспомагательную таблицу, благодаря которой я могу проанализировать пвторяющиеся заголовки на предмет их адекватности. из самых популярных заголовков вручную промечаю неинформативные (\"главная\" \"страница\", \"заголовок\" и пр). Отфильтровываю их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cc02ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "help_pivot = df.pivot_table(index ='title',values='index',aggfunc='count').sort_values(by='index',ascending=False)\n",
    "help_pivot = help_pivot.reset_index()\n",
    "help_pivot.columns=['title','title_count']\n",
    "help_pivot=help_pivot.loc[help_pivot['title_count']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af13031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "99c56d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = []\n",
    "for i in range(1,13),range(14,25),range(27,29), range(30,32),range(36,42),[43,45,46,48,49,50,51,52],range(54,60),range(62,66),[67,68,70],range(73,75),range(76,80),range(82,84), range(85,94), range(100,104),range(105,107), [109],range(111,115),range(116,119),range(121,128),range(129,136),[155,158,160],range(161,166),[167,173,177,185,186,187,190,192,193,196,198,204,208,209,213,214,218,220,221,224,229,232,233,235,237,239,240,241,249,252,254,255,256,259,260]:\n",
    "    for j in i:       \n",
    "        f.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0bad11a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['title'].isin(help_pivot.loc[f,'title']),'is_parced']=-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "47399ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['title'].isin(help_pivot.loc[f,'title']),'title']=\\\n",
    "df.loc[df['title'].isin(help_pivot.loc[f,'title']),'url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9837070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаю таблицу со всеми url и кол-вом ползоватлей, заходивших на них\n",
    "full_url = pd.read_csv('urls.csv')\n",
    "full_url.columns = ['url','us_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb74c93",
   "metadata": {},
   "source": [
    "Затем добавляю отсусвующие url и формирую таблицу для дальнейшей работы с тайтлами и кластеризацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "13548b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy()\n",
    "full_url=full_url.set_index('url')\n",
    "df2=df2.set_index('url')\n",
    "result = full_url.join(df2)\n",
    "result=result.reset_index()\n",
    "result = result[['url','us_count','title','is_parced']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "08f83e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[result['title'].isna(),'is_parced'] = -2\n",
    "result.loc[result['title'].isna(),'title'] = result.loc[result['title'].isna(),'url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ab460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "af6ebb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=result.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "fad629ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('titles_sib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b769d480",
   "metadata": {},
   "source": [
    "## Следующий этап работы см. файл : mts_ml_cup_sibrikova_main_body.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e5e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
